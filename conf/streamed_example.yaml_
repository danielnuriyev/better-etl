{
  "t": time.time(),
  "table": "table"
}
name: streamed_example
# schedule: 0-59 * * * *
retry:
  max: 2
  delay: 1
  backoff: exponential
  lookback_minutes: 1
  notifier:
    class: better_etl.notifiers.notifier.Notifier
resources:
  notifier:
    class:  better_etl.notifiers.notifier.Notifier
  cache:
    class:  better_etl.caches.S3Cache
    bucket: <bucket>
    path:   jobs/streamed_example
ops:
  - name: check_previous_run
    package:  better_etl.ops
    class:    Utils
    method:   check_previous_run
  - name: get_secret
    after:
      - check_previous_run
    package:  better_etl.ops
    class:    AWSSecretsManager
    method:   get_secret
    config:
      secret_name: <secret>
  - name: get_batches
    after:
      - get_secret
    package:  better_etl.ops
    class:    MySQL
    method:   get_batches
    config:
      host:     <host>
      database: <db>
      table:    $table
      batch:    1000000
      batches:  1
  - name: store_batches
    after:
      - get_batches
    package:  better_etl.ops
    class:    AWSS3
    method:   store
    config:
      bucket: <bucket>
      path:   tables/$table
      condition: >
        len(batch["data"]) > 0 and
        len(batch["data"]) <= 1000000
  - name: find_last_keys
    after:
      - store_batches
    package: better_etl.ops
    class: Utils
    method: find_last_keys
  - name: store_last_keys
    after:
      - find_last_keys
    package: better_etl.ops
    class: Cache
    method: put
    config:
      get_key: last_keys
      put_key: cache_key
  - name: compact
    after:
      - store_batches
    package: better_etl.ops
    class: Parquet
    method: compact
    config:
      bucket: <bucket>
      path:   <path>
      compact_path: True
      max_memory: 4GiB
      max_file_size : 128MiB
